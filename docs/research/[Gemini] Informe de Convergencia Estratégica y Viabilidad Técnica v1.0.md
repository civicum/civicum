Aqu√≠ tienes el **Informe de Convergencia Estrat√©gica y Viabilidad T√©cnica v1.0**.

Este documento integra, depura y operacionaliza los hallazgos de los tres informes de expertos que suministraste (*Arquitectura Soberana*, *Resumen Ejecutivo* y *An√°lisis Kimi K2.5*), contrast√°ndolos con la realidad de tu infraestructura actual (VPS 11GB RAM / AMD EPYC).

---

# INFORME T√âCNICO: CONVERGENCIA ESTRAT√âGICA PARA CIVICUM AI

**Fecha:** 08 Febrero 2026
**Destinatario:** Daniel (CTO Civicum)
**Infraestructura Objetivo:** VPS Soberano (11GB RAM Libre, CPU AMD EPYC, No-GPU)
**Fuentes:** Informe Kimi K2.5 (MoE), Informe IA Local (AMD/Soberan√≠a), Resumen Ejecutivo (Stacks).

---

## 1. RESUMEN EJECUTIVO Y VEREDICTO

Tras el an√°lisis cruzado de las tres fuentes y la capacidad instalada, se concluye que **es inviable** replicar la arquitectura de grandes modelos (Kimi K2.5 / Llama-70B) por restricciones f√≠sicas de VRAM. Sin embargo, se identific√≥ una **"Ruta √ìptima de Eficiencia"** que combina la inteligencia ligera de **Qwen 2.5**, la arquitectura de memoria de archivos (File-First) y optimizaciones espec√≠ficas para procesadores AMD.

**La estrategia ganadora es:** Infraestructura CPU-Only optimizada con NUMA, utilizando un modelo de 7B par√°metros cuantizado y un sistema de "auditor√≠a" (NLI) externo para garantizar la veracidad.

---

## 2. AN√ÅLISIS DE MODELOS (EL CEREBRO)

### A. El "Gigante" Descartado: Kimi K2.5 (MoE)

* **Fuente:** *Kimi K2.5 + Ollama + OpenClaw VPS.md*
* **An√°lisis:** Aunque Kimi ofrece razonamiento de nivel GPT-4 activando "solo" 32B par√°metros, requiere tener **1 Bill√≥n de par√°metros residentes en memoria**.
* **Restricci√≥n F√≠sica:** Exige >300 GB de VRAM. Tu VPS tiene 11 GB de RAM de sistema.
* **Decisi√≥n:** üõë **DESCARTADO para Inferencia.**
* **Valor Rescatado:** Se adopta su filosof√≠a de **"Memoria basada en Archivos"**. En lugar de bases de datos vectoriales pesadas en RAM, OpenClaw leer√° directamente archivos Markdown planos, lo cual es nativo, r√°pido y consume casi 0 RAM.

### B. La Competencia de los 7B: Mistral vs. Llama vs. Qwen

* **Fuente:** *Resumen ejecutivo* y *IA Local para Soporte C√≠vico*.
* **Evaluaci√≥n:**
* **Llama 3 (Meta):** Excelente generalista, pero propenso a ser "verboso" (gasta tokens innecesarios) y con licencia restrictiva en ciertos usos comerciales.
* **Mistral v0.2:** Recomendado en el "Resumen Ejecutivo" por estabilidad. Sin embargo, benchmarks de 2025 indican que se queda corto en razonamiento complejo y manejo de JSON.
* **Qwen 2.5 (7B Instruct):** Recomendado en el informe de "IA Local".
* *Ventaja 1:* Supera a Llama 3 en matem√°ticas y codificaci√≥n (clave para que OpenClaw opere el sistema).
* *Ventaja 2:* Soporte nativo de **System Prompts agresivos** (necesario para Aegis).
* *Ventaja 3:* Contexto de 128k tokens (aunque usaremos menos por RAM).




* **Decisi√≥n:** ‚úÖ **SELECCIONADO: Qwen 2.5 7B (Cuantizaci√≥n q4_k_m).**
* *Consumo:* ~4.8 GB RAM. Deja 6 GB libres para el sistema y contexto.



---

## 3. OPTIMIZACI√ìN DE HARDWARE (EL CUERPO)

### A. El Factor AMD EPYC (NUMA Awareness)

* **Fuente:** *IA Local para Soporte C√≠vico.md*
* **Hallazgo Cr√≠tico:** Los procesadores AMD EPYC (que usa tu VPS) funcionan con "Chiplets". Si el proceso de la IA salta de un chiplet a otro, la latencia se dispara.
* **Soluci√≥n T√©cnica:** Implementar `numactl` en el contenedor Docker. Esto "ancla" el proceso de Ollama a los n√∫cleos f√≠sicos m√°s cercanos a la memoria RAM asignada.
* **Impacto:** Mejora de velocidad estimada del **20-30%** sin costo financiero.

### B. Presupuesto de RAM (Zero-Swap Policy)

Para evitar que el servidor use el disco duro como RAM (lo que matar√≠a la velocidad de la IA), definimos este presupuesto estricto basado en tus 11GB libres:

| Componente | Asignaci√≥n | Estado |
| --- | --- | --- |
| **Modelo (Qwen 2.5 q4)** | 5.0 GB | Residente (Fijo) |
| **Contexto (Conversaci√≥n)** | 3.0 GB | Din√°mico (Ventana de 4k-8k tokens) |
| **OpenClaw (Node/Python)** | 1.0 GB | Operaci√≥n Ag√©ntica |
| **Sistema Operativo/Aegis** | 2.0 GB | Kernel, Docker, Nginx |
| **Total** | **11.0 GB** | **L√≠mite Seguro** |

---

## 4. ARQUITECTURA DE SEGURIDAD (EL ESCUDO)

### A. Verificaci√≥n "Evidence-Only" (NLI)

* **Fuente:** *Resumen ejecutivo (Recomendaci√≥n Top 3)*
* **Problema:** Los modelos de 7B alucinan. Pueden inventar leyes.
* **Soluci√≥n:** Implementar un **M√≥dulo Arbiter**.
* No basta con que Qwen genere una respuesta.
* Se usar√° un modelo min√∫sculo (tipo `XLM-RoBERTa`, ~300MB) que funciona como un "Juez de Verdad".
* *Flujo:* OpenClaw genera respuesta -> Arbiter compara con el documento legal -> Si no hay coincidencia sem√°ntica -> **Bloquea la respuesta**.


* **Decisi√≥n:** Implementar en Fase 2 como middleware obligatorio.

### B. Ofuscaci√≥n de PII (Protecci√≥n de Datos)

* **Fuente:** *IA Local para Soporte C√≠vico* (Ref: Microsoft Presidio).
* **Adaptaci√≥n:** Levantar la suite completa de Microsoft Presidio requiere Java y mucha RAM.
* **Soluci√≥n "Zero-Cost":** Implementaremos un **Middleware de Regex Optimizado** en Node.js dentro de OpenClaw.
* Detectar√° patrones de RUT chileno (`\d{1,2}\.\d{3}\.\d{3}-[\dkK]`) y emails.
* Reemplazo autom√°tico por tokens `[DATOS_SENSIBLES]` *antes* de que el texto toque a Qwen.
* Esto cumple la normativa de privacidad sin el costo de recursos de Presidio.



---

## 5. HOJA DE RUTA T√âCNICA INMEDIATA

Basado en esta convergencia, las instrucciones para la terminal (Fase 1) se actualizan a:

1. **Motor:** Descargar `ollama pull qwen2.5:7b-instruct-q4_k_m` (M√°ximo balance IQ/RAM).
2. **Configuraci√≥n:** Editar el `docker-compose.yml` para inyectar optimizaciones de CPU (si Docker lo permite en el host) o configurar los *threads* de Ollama para coincidir con los vCPUs f√≠sicos.
3. **Memoria:** Crear la estructura de carpetas para la "Memoria de Archivos" (Markdown) que Kimi nos inspir√≥.

### Conclusi√≥n Final

Daniel, la investigaci√≥n confirma que tu instinto de usar un VPS local es t√©cnicamente viable **solo si** somos quir√∫rgicos con la elecci√≥n del modelo. **Qwen 2.5** es la llave maestra. Descartamos la fuerza bruta (Kimi/Llama-70B) y apostamos por la **eficiencia t√°ctica** (Qwen + NUMA + Evidence Verification).

Este enfoque nos da un sistema **Soberano, R√°pido y Privado** que cabe en tu bolsillo y en tu servidor.

¬øProcedemos a instalar **Qwen 2.5**?